\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{todonotes}

\title{
COMPSCI 403: Project Proposal \\ 
A Robotic Etch-A-Sketch
}
\author{
Alex Karle : \texttt{akarle@umass.edu}  \\ 
Alex Guerriero : \texttt{aguerriero@umass.edu}  \\ 
Jagath Jai Kumar : \texttt{jjaikumar@umass.edu} \\ 
Matthew LeBlanc : \texttt{mkleblanc@umass.edu}
}

\begin{document}
\maketitle

\begin{abstract}
In this project, we will design a user-controlled drawing system that detects a humans hands movements and correlates them to controls for a robot that will draw on a white board. The tool will consist of two sub-systems: a human controller observation system and a robot drawing system. The observation element will consist of one Kinect to monitor the human user located 2$m$ from the sensor. The robot will draw on a large piece of paper on the ground based on the human's hands positioning collected by the sensor. The right hand of the user will determine the direction and velocity of the forward/backward motion of the robot, and the left hand will determine the direction and angular velocity of the clockwise/counterclockwise rotation. The robot will be confined to the area of a 1$m^2$ piece of white paper laid on the ground, so it will not draw outside of the bounds edges of the paper.
\end{abstract}

\section{Project Goals}

The goals of the project include:

\begin{enumerate}
\item Collect Depth Image from single Kinect observing a human "controller". Convert DepthImage to PointCloud.

\item Calibration step (figure out where the human is in the point cloud)--filter the point cloud for human, under the assumption that the human is the closest thing to the Kinect, and no other objects are in view that are closer.

\item Recognize the hand positions in point cloud, specifically the positioning of the hands with respect to the torso in the $y$ direction. The positioning of the hands, and NOT the gestures the hands are making will be used to control the robot.

\item Convert hand positions to forward/backward movement and clockwise/counterclockwise rotation commands for the robot, like an etch-a-sketch. The y coordinates of hands (with respect to the torso level being the origin), will indicate speed of movement or rotation (by magnitude), and direction (by sign).

\item Compute robot navigation (make the robot move or rotate in direction of controls) based on these aforementioned computed commands and according to the pre-specified boundaries (edges of the physical space the robot should move in).

\item \textbf{Stretch goals:} Recognize gesture for lifting up / putting down marker, and have the robot act accordingly using a basic motor of some sort. If that works out, working with multiple different colors of markers
\end{enumerate}

\newpage
\section{Work Plan}

\textbf{Overview:}
\begin{itemize}
\item Alex G. : Focus on turning the depth image into a point cloud, filtering out the human in the point cloud, and determining the quadrants which correspond to forward and backwards movement, and counterclockwise and clockwise rotation.
\item Alex K. : Focused on finding the hand positions ($x$, $y$) for each hand after calibration and sending these to Jagath.
\item Jagath : Focused on converting Human hand position ($x$, $y$) coordinate to ($v$, $\omega$) pair for robot and sending information wireless-ly
\item Matthew : Receive wireless movement information and use the ($v$, $\omega$) pair transmitted to determine how the robot should move
\end{itemize}

\subsection{Weeks 1 \& 2}

\textbf{Week 1 (3/6)} \\
Alex G. : Get color point cloud from depth image and test using handcrafted depth images\\
Alex K. : Synthesize or obtain artificial filtered human point clouds\\
Jagath : Determine best mapping method for converting ($x$, $y$) coordinate to ($v$, $\omega$) pair \\
Matthew : Determine limits of robot movement \\
\textbf{Week 2 (3/13)} \\
Alex G. : Finalize color point cloud and work on filtering human from point cloud \\
Alex K. : Determine and experiment with extracting hand positioning from test human point clouds\\
Jagath : Write code to convert human ($x$, $y$) to robot ($v$, $\omega$) \\
Matthew :  Write code to safely move the robot according to input ($v$, $\omega$) coordinate pair \\

\subsection{Weeks 3 \& 4}
\textbf{Week 3 (3/20)} \\
Alex G. : Test to make sure human is being filtered out correctly from point cloud.\\
Alex K. : Polish approach to extracting hand positioning\\
Jagath : Write code to transmit data wireless-ly from Kinect-laptop to Turtlebot\\
Matthew : Write code to receive wireless data from the Kinect-laptop \\
\textbf{Week 4 (3/27)} \\
Alex G. : Work with Alex K on taking real Kinect point clouds, and fine tuning algorithm \\
Alex K. : Work with Alex G. on taking real Kinect point clouds, and fine tuning position-finding algorithms\\
Jagath : Work with Matthew to test wireless transmission and reception code\\
Matthew :  Work with Jagath to test the reception and movement \\

\subsection{Weeks 5 \& 6}

\textbf{Week 5 (4/2)} \\
Alex G. : Capture realistic rosbag files of human point clouds with sample gestures for testing purposes and work on intermediate report with Matt\\
Alex K. : Write tests for hand positioning. Work with Jagath on transition to robot\\
Jagath : Test ($x$, $y$) to ($v$, $\omega$) code and refine to work on actual robot with Alex K.\\
Matthew : Work on intermediate progress report and also work out any bugs that come up \\
\textbf{Week 6 (4/9)} \\
Alex G. : Integration Tests with Alex K. \\
Alex K. : Run tests, and adapt as necessary (there will likely be issues). Overall integration tests\\
Jagath : Control integration tests to complete preliminary robot design\\
Matthew : Control integration tests for beginning robot design \\

\subsection{Weeks 7 \& 8}

\textbf{Week 7 (4/16)} \\
Alex G. : Gesture integration tests \\
Alex K. : Gesture integration tests and refinement.\\
Jagath : Refine and improve coordinate conversions and look into completing stretch goals\\
Matthew : Fine tune movement commands and work on stretch goals if there is time\\
\textbf{Week 8 (4/23)} \\
Alex G. : Work on final report and project demo\\
Alex K. : Work on final report and project demo\\
Jagath : Overall integration tests and stretch goals\\
Matthew : Overall integration tests and stretch goals \\

\section{Evaluation Metrics}
We propose to evaluate our project based on the successful completion of the following tests:

\begin{enumerate}
\item \textbf{Depth Image Collection and Conversion}

\textbf{Binary}: Using a handmade depth image with pre-calculated results test to make sure that it is correctly converted to a point cloud

\item \textbf{Calibration and Filtering Humans}

\textbf{Accuracy}: Using a pre-made point cloud with human standing at fixed distance from Kinect calculate different in depth for expected human position and calculated depth of human.

\textbf{Binary}: Assert that point cloud can filter 3 humans of different shapes and sizes at different distances away from the Kinect.

\item \textbf{Hand Position Recognition}

\textbf{Accuracy}: A test set should be collected where the height of the controller is known. Using this height info, we should be able to actually determine the real world heights of the hands, and thus extrapolate the expected coordinates in our coordinate system (given correct calibration, a separate test).

\item \textbf{Hand Position to Control Sequences}: 

\textbf{Binary}: Send ($x$,$y$) coordinates for several different hand positions and test for successful conversions to correct ($v$, $\omega$) coordinate pairs.

\item \textbf{Control Sequences to Robot Navigation}

\textbf{Binary}: Send commands for each possible pair of directions (i.e. forwards and rotate left, backwards and no rotation, etc.) and check that it moves in the appropriate direction
\end{enumerate}

\section{Risk Management}

\begin{enumerate}
\item Recognizing hand positions from the point cloud has the potential to go wrong. If this is the case we can use a library downloaded from the Internet to convert a Kinect image to a skeleton (i.e. AirKinect library or some alternative).
\item If the magnitude is difficult to detect based on hand position the program could instead only determine the quadrants that arms are in and have a fixed forward/backward and rotational speed.
\item If hand recognition is too difficult, putting colored sticky notes on the hands to improve recognition would make the task much simpler.
\end{enumerate}


\end{document}